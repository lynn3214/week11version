"""
Main CLI entry point for dolphin click detection pipeline.
Changes:
1. Fix batch-detect logic for saving audio segments
2. Fix build-dataset input paths and SNR mixing logic
3. Add 'collect-clicks' command
4. Add debug output and SNR validation
"""

import argparse
from pathlib import Path
import sys
import shutil  
from tqdm import tqdm 
import random

from utils.config import load_config
from utils.logging.logger import ProjectLogger
from utils.audio_io.manifest import scan_audio_files
#from utils.preprocessing.resample_and_filter import preprocess_audio_file
from detection.candidate_finder.dynamic_threshold import AdaptiveDetector, DetectionParams
from detection.segmenter.cropper import ClickSegmenter
from detection.features_event.event_stats import EventStatsExtractor, save_event_stats_csv
from detection.train_builder.cluster import TrainBuilder, save_trains_csv
from detection.fusion.decision import FusionDecider, FusionConfig
from detection.export.writer import ExportWriter
from training.dataset.segments import DatasetBuilder
from training.augment.pipeline import AugmentationPipeline
from models.cnn1d.model import create_model
from models.cnn1d.inference import ClickDetectorInference
from training.train.loop import Trainer, create_dataloaders
from training.eval.report import EvaluationReporter

import numpy as np
import torch
import soundfile as sf
import pandas as pd

import matplotlib
matplotlib.use('Agg')  # éGUIåç«¯
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, classification_report,
    roc_curve, auc, precision_recall_curve
)


def setup_argparse():
    """Setup command line argument parser."""
    parser = argparse.ArgumentParser(
        description='Dolphin Click Detection Pipeline'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Command to run')
    
    # Scan command
    scan_parser = subparsers.add_parser('scan', help='Scan audio files')
    scan_parser.add_argument('--input-dir', type=str, required=True,
                            help='Input directory to scan')
    scan_parser.add_argument('--output', type=str, required=True,
                            help='Output manifest file')
    
    # Detect command
    detect_parser = subparsers.add_parser('detect', help='Detect click candidates')
    detect_parser.add_argument('--input', type=str, required=True,
                              help='Input audio file')
    detect_parser.add_argument('--output-dir', type=str, required=True,
                              help='Output directory')
    detect_parser.add_argument('--config', type=str, default='configs/detection.yaml',
                              help='Detection config file')
    
    # ========== Added: collect-clicks command ==========
    collect_parser = subparsers.add_parser(
        'collect-clicks',
        help='Collect all click segments into a single directory'
    )
    collect_parser.add_argument('--input', type=str, required=True,
                               help='Input directory (detection_results/audio)')
    collect_parser.add_argument('--output', type=str, required=True,
                               help='Output directory for collected clicks')
    collect_parser.add_argument('--verbose', '-v', action='store_true')
    
    # Batch detect command
    batch_detect_parser = subparsers.add_parser(
        'batch-detect', 
        help='Batch detect clicks in directory'
    )
    batch_detect_parser.add_argument('--input-dir', type=str, required=True,
                                    help='Input directory containing wav files')
    batch_detect_parser.add_argument('--output-dir', type=str, required=True,
                                    help='Output directory for results')
    batch_detect_parser.add_argument('--config', type=str, default='configs/detection.yaml',
                                    help='Detection config file')
    batch_detect_parser.add_argument('--save-audio', action='store_true',
                                    help='Save extracted click segments')
    batch_detect_parser.add_argument('--recursive', action='store_true',
                                    help='Search recursively for wav files')
    batch_detect_parser.add_argument('--segment-ms', type=float, default=120.0,
                                    help='Segment length in milliseconds (default: 120)')
    
    # Trains command
    trains_parser = subparsers.add_parser('trains', help='Build click trains')
    trains_parser.add_argument('--events-csv', type=str, required=True,
                              help='Events CSV file')
    trains_parser.add_argument('--output', type=str, required=True,
                              help='Output trains CSV')
    trains_parser.add_argument('--config', type=str, default='configs/detection.yaml',
                              help='Detection config file')
    
    # ========== Modified: build-dataset update parameter descriptions ==========
    dataset_parser = subparsers.add_parser('build-dataset',
                                          help='Build training dataset with SNR mixing')
    dataset_parser.add_argument('--events-dir', type=str, required=True,
                               help='Directory containing click wav files (augmented_clicks)')
    dataset_parser.add_argument('--noise-dir', type=str, required=True,
                               help='Directory containing noise segments (noise_train_segs)')
    dataset_parser.add_argument('--output-dir', type=str, required=True,
                               help='Output dataset directory')
    dataset_parser.add_argument('--config', type=str, default='configs/training.yaml',
                               help='Training config file')
    dataset_parser.add_argument('--save-wav', action='store_true',
                               help='Save mixed samples as wav files for inspection')
    dataset_parser.add_argument('--verbose', '-v', action='store_true')
    
    # Train command
    train_parser = subparsers.add_parser('train', help='Train CNN model')
    train_parser.add_argument('--dataset-dir', type=str, required=True,
                             help='Dataset directory')
    train_parser.add_argument('--output-dir', type=str, required=True,
                             help='Output directory for checkpoints')
    train_parser.add_argument('--config', type=str, default='configs/training.yaml',
                             help='Training config file')
    train_parser.add_argument('--verbose', '-v', action='store_true',
                         help='Verbose logging')
    
    # Eval command
    eval_parser = subparsers.add_parser('eval', help='Evaluate model')
    eval_parser.add_argument('--checkpoint', type=str, required=True,
                            help='Model checkpoint path')
    eval_parser.add_argument('--dataset-dir', type=str, required=True,
                            help='Test dataset directory')
    eval_parser.add_argument('--output-dir', type=str, required=True,
                            help='Output directory for reports')

    # Eval-wav command (æ–°å¢)
    eval_wav_parser = subparsers.add_parser(
        'eval-wav', 
        help='Evaluate model on wav files (file-level classification)'
    )
    eval_wav_parser.add_argument('--checkpoint', type=str, required=True,
                                help='Model checkpoint path')
    eval_wav_parser.add_argument('--positive-dir', type=str, 
                                default='data/test_resampled',
                                help='Directory with files containing clicks')
    eval_wav_parser.add_argument('--negative-dir', type=str,
                                default='data/noise_resampled',
                                help='Directory with noise files')
    eval_wav_parser.add_argument('--output-dir', type=str, required=True,
                                help='Output directory for results')
    eval_wav_parser.add_argument('--config', type=str, 
                                default='configs/eval_wav.yaml',
                                help='Evaluation config file')
    eval_wav_parser.add_argument('--file-threshold', type=float,
                                help='Window-level threshold (overrides config)')
    eval_wav_parser.add_argument('--min-positive-ratio', type=float,
                                help='Min positive ratio (overrides config)')
    
    # Export command
    export_parser = subparsers.add_parser('export', help='Export final detections')
    export_parser.add_argument('--input', type=str, required=True,
                              help='Input audio file')
    export_parser.add_argument('--checkpoint', type=str, required=True,
                              help='Model checkpoint')
    export_parser.add_argument('--output-dir', type=str, required=True,
                              help='Output directory')
    export_parser.add_argument('--config', type=str, default='configs/inference.yaml',
                              help='Inference config file')
    
    return parser


def cmd_scan(args):
    """Execute scan command."""
    logger = ProjectLogger()
    logger.info(f"Scanning directory: {args.input_dir}")
    
    manifest = scan_audio_files(
        Path(args.input_dir),
        extensions=['.wav'],
        recursive=True
    )
    
    manifest.to_csv(args.output, index=False)
    logger.info(f"Manifest saved to {args.output}")
    logger.info(f"Found {len(manifest)} audio files")


def cmd_detect(args):
    """Execute detect command."""
    logger = ProjectLogger()
    config = load_config(args.config)
    
    logger.info(f"Detecting clicks in: {args.input}")
    
    # Load audio
    audio, sr = sf.read(args.input)
    
    # Initialize detector
    params = DetectionParams(
        tkeo_threshold=config['thresholds']['tkeo_z'],
        ste_threshold=config['thresholds']['ste_z'],
        hfc_threshold=config['thresholds']['hfc_z'],
        high_low_ratio_threshold=config['thresholds']['high_low_ratio'],
        envelope_width_min=config['envelope']['width_min_ms'],
        envelope_width_max=config['envelope']['width_max_ms'],
        spectral_centroid_min=config['thresholds']['spectral_centroid_min'],
        refractory_ms=config['refractory_ms']
    )
    
    detector = AdaptiveDetector(sample_rate=sr, params=params)
    
    # Detect
    candidates = detector.batch_detect(
        audio,
        chunk_duration=config['batch']['chunk_duration_s'],
        overlap=config['batch']['overlap_s']
    )
    
    logger.info(f"Detected {len(candidates)} candidates")
    
    # Extract event statistics
    stats_extractor = EventStatsExtractor(sample_rate=sr)
    stats_list = []
    
    for candidate in candidates:
        stats = stats_extractor.extract_event_stats(audio, candidate)
        stats_list.append(stats)
    
    # Save results
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    events_csv = output_dir / 'events.csv'
    save_event_stats_csv(stats_list, events_csv)
    
    logger.info(f"Events saved to {events_csv}")

# ========== Added: collect-clicks command implementation ==========
def cmd_collect_clicks(args):
    """æ”¶é›†æ‰€æœ‰ click ç‰‡æ®µåˆ°å•ä¸ªç›®å½•."""
    logger = ProjectLogger()
    logger.info("å¼€å§‹æ”¶é›† click ç‰‡æ®µ...")
    
    input_dir = Path(args.input)
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰ wav æ–‡ä»¶
    wav_files = list(input_dir.rglob('*.wav'))
    logger.info(f"æ‰¾åˆ° {len(wav_files)} ä¸ª click ç‰‡æ®µ")
    
    if not wav_files:
        logger.error(f"æœªæ‰¾åˆ°ä»»ä½• WAV æ–‡ä»¶: {input_dir}")
        return
    
    # å¤åˆ¶åˆ°è¾“å‡ºç›®å½•(é‡å‘½åä»¥é¿å…å†²çª)
    for i, wav_file in enumerate(tqdm(wav_files, desc="æ”¶é›†ç‰‡æ®µ")):
        # ä¿ç•™åŸå§‹æ–‡ä»¶å‰ç¼€(æ¥è‡ªçˆ¶ç›®å½•)
        parent_name = wav_file.parent.name
        new_name = f"{parent_name}_{wav_file.name}"
        shutil.copy2(wav_file, output_dir / new_name)
    
    logger.info(f"âœ… æ”¶é›†å®Œæˆ,ä¿å­˜åˆ° {output_dir}")

# ========== Modified: batch-detect save audio segments ==========
def cmd_batch_detect(args):
    """Execute batch-detect command."""
    logger = ProjectLogger()
    config = load_config(args.config)
    
    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # âœ… ç®€åŒ–å‚æ•°è·å–
    segment_ms = args.segment_ms
    
    # æ‰«æ wav æ–‡ä»¶
    logger.info(f"æ‰«æç›®å½•: {input_dir}")
    if args.recursive:
        wav_files = list(input_dir.rglob('*.wav'))
    else:
        wav_files = list(input_dir.glob('*.wav'))
    
    logger.info(f"æ‰¾åˆ° {len(wav_files)} ä¸ª wav æ–‡ä»¶")
    
    if not wav_files:
        logger.error(f"æœªæ‰¾åˆ°ä»»ä½• WAV æ–‡ä»¶: {input_dir}")
        return
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    params = DetectionParams(
        tkeo_threshold=config['thresholds']['tkeo_z'],
        ste_threshold=config['thresholds']['ste_z'],
        hfc_threshold=config['thresholds']['hfc_z'],
        high_low_ratio_threshold=config['thresholds']['high_low_ratio'],
        envelope_width_min=config['envelope']['width_min_ms'],
        envelope_width_max=config['envelope']['width_max_ms'],
        spectral_centroid_min=config['thresholds']['spectral_centroid_min'],
        refractory_ms=config['refractory_ms'],
        enable_transient_filter=config['transient'].get('enable_filter', True),
        min_dolphin_likelihood=config['transient'].get('min_dolphin_likelihood', 0.3)
    )
    
    all_stats = []
    total_candidates = 0
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for wav_file in tqdm(wav_files, desc="æ£€æµ‹ clicks"):
        try:
            audio, sr = sf.read(wav_file)
            file_id = wav_file.stem
            
            # æ£€æµ‹
            detector = AdaptiveDetector(sample_rate=sr, params=params)
            candidates = detector.batch_detect(
                audio,
                chunk_duration=config['batch']['chunk_duration_s'],
                overlap=config['batch']['overlap_s']
            )
            
            total_candidates += len(candidates)
            logger.info(f"{wav_file.name}: æ£€æµ‹åˆ° {len(candidates)} ä¸ªå€™é€‰")
            
            # æå–ç»Ÿè®¡ä¿¡æ¯
            stats_extractor = EventStatsExtractor(sample_rate=sr)
            for candidate in candidates:
                stats = stats_extractor.extract_event_stats(audio, candidate)
                stats['file_id'] = file_id
                stats['source_file'] = str(wav_file)
                all_stats.append(stats)
            
            # ä¿å­˜éŸ³é¢‘ç‰‡æ®µ
            if args.save_audio and candidates:
                audio_dir = output_dir / 'audio' / file_id
                audio_dir.mkdir(parents=True, exist_ok=True)
                
                segment_samples = int(segment_ms * sr / 1000)
                
                for i, candidate in enumerate(candidates):
                    # æå–å›ºå®šé•¿åº¦ç‰‡æ®µ(ä»¥å³°å€¼ä¸ºä¸­å¿ƒ)
                    half_window = segment_samples // 2
                    start_idx = max(0, candidate.peak_idx - half_window)
                    end_idx = min(len(audio), candidate.peak_idx + half_window)
                    
                    segment = audio[start_idx:end_idx]
                    
                    # å¦‚éœ€å¡«å……
                    if len(segment) < segment_samples:
                        pad_left = (segment_samples - len(segment)) // 2
                        pad_right = segment_samples - len(segment) - pad_left
                        segment = np.pad(segment, (pad_left, pad_right), mode='reflect')
                    
                    # å½’ä¸€åŒ–(é¿å…åç»­å‰Šæ³¢)
                    peak_val = np.max(np.abs(segment))
                    if peak_val > 0:
                        segment = segment / peak_val * 0.95  # ç•™ 5% ä½™é‡
                    
                    # ä¿å­˜
                    timestamp_ms = int(candidate.peak_time * 1000)
                    filename = f"click_{i:04d}_{timestamp_ms:08d}ms.wav"
                    sf.write(audio_dir / filename, segment, sr)
                
        except Exception as e:
            logger.error(f"å¤„ç† {wav_file} æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # ä¿å­˜æ±‡æ€» CSV
    if all_stats:
        csv_path = output_dir / 'all_events.csv'
        save_event_stats_csv(all_stats, csv_path)
        logger.info(f"âœ… ä¿å­˜ {len(all_stats)} ä¸ªäº‹ä»¶åˆ° {csv_path}")
        logger.info(f"âœ… å¤„ç†å®Œæˆ! æ€»å…±æ£€æµ‹åˆ° {total_candidates} ä¸ª click å€™é€‰")
    else:
        logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°ä»»ä½•äº‹ä»¶")


def cmd_trains(args):
    """Execute trains command."""
    logger = ProjectLogger()
    config = load_config(args.config)
    
    logger.info(f"Building trains from: {args.events_csv}")
    
    # Load events (this is simplified - in practice you'd reconstruct ClickCandidate objects)
    events_df = pd.read_csv(args.events_csv)
    
    # For this example, we'll need to import the candidates from somewhere
    # This is a placeholder - actual implementation would need to deserialize candidates
    logger.warning("Train building from CSV requires candidate objects - not fully implemented")
    
    # Initialize train builder
    train_builder = TrainBuilder(
        min_ici_ms=config['train']['min_ici_ms'],
        max_ici_ms=config['train']['max_ici_ms'],
        min_train_clicks=config['train']['min_train_clicks']
    )
    
    # Build trains (placeholder)
    # trains = train_builder.build_trains(candidates)
    
    logger.info("Train building command - implementation depends on serialization format")


def cmd_build_dataset(args):
    """Execute build-dataset commandï¼ˆä¿®æ­£ç‰ˆ - åˆ†ç¦»è®­ç»ƒ/éªŒè¯é›†ç”Ÿæˆï¼‰."""
    logger = ProjectLogger()
    config = load_config(args.config)
    
    logger.info("=" * 60)
    logger.info("æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆclick trainåºåˆ— + SNRæ··åˆï¼‰")
    logger.info("=" * 60)

    # ========== æ¸…ç†ä¸Šæ¬¡è¾“å‡º ==========
    output_dir = Path(args.output_dir)
    if output_dir.exists():
        logger.info(f"æ¸…ç†ä¸Šæ¬¡è¾“å‡º: {output_dir}")
        train_dir = output_dir / 'train'
        if train_dir.exists():
            shutil.rmtree(train_dir)
        val_dir = output_dir / 'val'
        if val_dir.exists():
            shutil.rmtree(val_dir)
        debug_dir = output_dir / 'debug_wavs'
        if debug_dir.exists():
            shutil.rmtree(debug_dir)
            
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ========== åˆå§‹åŒ–é…ç½® ==========
    dataset_config = config['dataset']
    sample_rate = config.get('sample_rate', 44100)
    window_ms = dataset_config.get('window_ms', 120.0)
    
    logger.info(f"æ ·æœ¬ç‡: {sample_rate} Hz")
    logger.info(f"ç»Ÿä¸€æ ·æœ¬é•¿åº¦: 500ms ({int(0.5 * sample_rate)} æ ·æœ¬)")
    
    builder = DatasetBuilder(
        sample_rate=sample_rate,
        window_ms=window_ms,
        random_offset_ms=dataset_config['random_offset_ms'],
        unified_length_ms=500.0
    )
    
    # åˆå§‹åŒ–å¢å¼ºå™¨
    augmentation_config = config.get('augmentation', {})
    augmenter = AugmentationPipeline(
        sample_rate=sample_rate,
        snr_range=tuple(augmentation_config.get('snr_range', [-5, 15])),
        time_shift_ms=augmentation_config.get('time_shift_ms', 10.0),
        amplitude_range=tuple(augmentation_config.get('amplitude_range', [0.8, 1.25])),
        apply_prob=augmentation_config.get('apply_prob', 0.8)
    )
    
    logger.info(f"\nå¢å¼ºè®¾ç½®:")
    logger.info(f"  SNRèŒƒå›´: {augmenter.snr_range} dB")
    logger.info(f"  æ—¶é—´åç§»: Â±{augmentation_config.get('time_shift_ms', 10.0)} ms")
    logger.info(f"  åº”ç”¨æ¦‚ç‡: {augmenter.apply_prob}")
    
    events_dir = Path(args.events_dir)
    noise_dir = Path(args.noise_dir)
    
    # ========== åŠ è½½å™ªå£°æ±  ==========
    logger.info(f"\nåŠ è½½å™ªå£°æ–‡ä»¶æ± ...")
    noise_files = list(noise_dir.rglob('*.wav'))
    
    if not noise_files:
        logger.error(f"æœªæ‰¾åˆ°å™ªå£°æ–‡ä»¶: {noise_dir}")
        return
    
    logger.info(f"æ‰¾åˆ° {len(noise_files)} ä¸ªå™ªå£°æ–‡ä»¶")
    
    # åŠ è½½å™ªå£°æ± ï¼ˆæœ€å¤š100ä¸ªæ–‡ä»¶ï¼‰
    max_noise_files = min(len(noise_files), 100)
    noise_pool = []
    selected_noise_files = random.sample(noise_files, max_noise_files)
    min_noise_length = int(0.5 * sample_rate)  # 500ms
    
    for noise_file in tqdm(selected_noise_files, desc="åŠ è½½å™ªå£°æ± "):
        try:
            noise_audio, sr = sf.read(noise_file)
            
            # é‡é‡‡æ ·
            if sr != sample_rate:
                import librosa
                noise_audio = librosa.resample(noise_audio, orig_sr=sr, target_sr=sample_rate)
            
            # è½¬å•å£°é“
            if noise_audio.ndim == 2:
                noise_audio = noise_audio.mean(axis=1)
            
            # ğŸ”§ ç¡®ä¿å™ªå£°è¶³å¤Ÿé•¿ï¼ˆå¦‚æœçŸ­äº500msï¼Œé‡å¤å¡«å……ï¼‰
            if len(noise_audio) < min_noise_length:
                repeats = int(np.ceil(min_noise_length / len(noise_audio)))
                noise_audio = np.tile(noise_audio, repeats)
                logger.debug(f"å™ªå£°æ–‡ä»¶ {noise_file.name} è¿‡çŸ­ï¼Œå·²é‡å¤å¡«å……")
            
            # RMSå½’ä¸€åŒ–åˆ°å›ºå®šæ°´å¹³
            rms = np.sqrt(np.mean(noise_audio**2))
            if rms > 1e-8:
                target_rms = 0.1
                noise_audio = noise_audio * (target_rms / rms)
            
            # å³°å€¼è£å‰ª
            peak = np.max(np.abs(noise_audio))
            if peak > 0.95:
                noise_audio = noise_audio / peak * 0.95
            
            noise_pool.append(noise_audio)
            
        except Exception as e:
            logger.error(f"åŠ è½½å™ªå£°å¤±è´¥ {noise_file}: {e}")
            continue

    logger.info(f"æˆåŠŸåŠ è½½ {len(noise_pool)} ä¸ªå™ªå£°ç‰‡æ®µï¼ˆå·²RMSå½’ä¸€åŒ–åˆ°0.1ï¼‰")
    
    if len(noise_pool) == 0:
        logger.error("å™ªå£°æ± ä¸ºç©ºï¼")
        return
    
    # ========== å‡†å¤‡Clickç´ ææ–‡ä»¶ ==========
    logger.info(f"\nå‡†å¤‡Clickç´ ææ–‡ä»¶...")
    positive_files = list(events_dir.rglob('*.wav'))

    if not positive_files:
        logger.error(f"æœªæ‰¾åˆ°clickæ–‡ä»¶: {events_dir}")
        return

    logger.info(f"æ‰¾åˆ° {len(positive_files)} ä¸ªclickç‰‡æ®µç”¨äºç»„å»ºtrain")

    # ========== ç”ŸæˆClick Trainåºåˆ—æ ·æœ¬ ==========
    train_config = dataset_config.get('click_train', {})
    enable_train = train_config.get('enable', True)

    if not enable_train:
        logger.error("âŒ å¿…é¡»å¯ç”¨click trainç”Ÿæˆï¼")
        return

    logger.info(f"\nç”ŸæˆClick Trainåºåˆ—æ ·æœ¬...")

    # ğŸ”§ æ–°å¢ï¼šè¯»å–è®­ç»ƒé›†å’ŒéªŒè¯é›†æ ·æœ¬æ•°
    n_train_samples = train_config.get('n_samples', 8000)
    n_val_samples = train_config.get('val_samples', 2000)
    train_length_ms = train_config.get('train_length_ms', 500.0)
    min_clicks = train_config.get('min_clicks', 2)
    max_clicks = train_config.get('max_clicks', 5)
    ici_range_ms = tuple(train_config.get('ici_range_ms', [10.0, 80.0]))

    logger.info(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {n_train_samples}")
    logger.info(f"  éªŒè¯é›†æ ·æœ¬æ•°: {n_val_samples}")
    logger.info(f"  Trainé•¿åº¦: {train_length_ms}ms")
    logger.info(f"  Clicksæ•°èŒƒå›´: {min_clicks}-{max_clicks}")
    logger.info(f"  ICIèŒƒå›´: {ici_range_ms} ms")

    # ğŸ”§ åˆ†åˆ«ç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
    logger.info(f"\nç”Ÿæˆè®­ç»ƒé›† click trains...")
    train_samples = builder.build_click_train_samples(
        click_files=positive_files,
        n_train_samples=n_train_samples,
        train_length_ms=train_length_ms,
        min_clicks=min_clicks,
        max_clicks=max_clicks,
        ici_range_ms=ici_range_ms,
        sample_rate=sample_rate,
        noise_pool=noise_pool,
        augmenter=augmenter
    )

    logger.info(f"\nç”ŸæˆéªŒè¯é›† click trains...")
    val_positive_samples = builder.build_click_train_samples(
        click_files=positive_files,
        n_train_samples=n_val_samples,
        train_length_ms=train_length_ms,
        min_clicks=min_clicks,
        max_clicks=max_clicks,
        ici_range_ms=ici_range_ms,
        sample_rate=sample_rate,
        noise_pool=noise_pool,
        augmenter=augmenter
    )

    logger.info(f"  è®­ç»ƒé›†æ­£æ ·æœ¬: {len(train_samples)}")
    logger.info(f"  éªŒè¯é›†æ­£æ ·æœ¬: {len(val_positive_samples)}")
    
    # ========== å¤„ç†è´Ÿæ ·æœ¬ ==========
    logger.info(f"\nå¤„ç†è´Ÿæ ·æœ¬...")
    
    balance_ratio = dataset_config.get('balance_ratio', 1.0)
    
    # è®­ç»ƒé›†è´Ÿæ ·æœ¬
    n_negative_train = int(len(train_samples) * balance_ratio)
    n_negative_per_file_train = max(1, n_negative_train // len(noise_files))
    
    # éªŒè¯é›†è´Ÿæ ·æœ¬
    n_negative_val = int(len(val_positive_samples) * balance_ratio)
    n_negative_per_file_val = max(1, n_negative_val // len(noise_files))
    
    logger.info(f"è®­ç»ƒé›†ç›®æ ‡è´Ÿæ ·æœ¬æ•°: {n_negative_train}")
    logger.info(f"éªŒè¯é›†ç›®æ ‡è´Ÿæ ·æœ¬æ•°: {n_negative_val}")
    
    train_negative_samples = []
    val_negative_samples = []
    
    # ğŸ”§ ä½¿ç”¨ä¸åŒçš„éšæœºç§å­ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„è´Ÿæ ·æœ¬ä¸é‡å 
    train_noise_files = noise_files[:int(len(noise_files) * 0.8)]  # 80%ç”¨äºè®­ç»ƒ
    val_noise_files = noise_files[int(len(noise_files) * 0.8):]    # 20%ç”¨äºéªŒè¯
    
    logger.info(f"è®­ç»ƒé›†ä½¿ç”¨ {len(train_noise_files)} ä¸ªå™ªå£°æ–‡ä»¶")
    logger.info(f"éªŒè¯é›†ä½¿ç”¨ {len(val_noise_files)} ä¸ªå™ªå£°æ–‡ä»¶")
    
    # ç”Ÿæˆè®­ç»ƒé›†è´Ÿæ ·æœ¬ï¼ˆæ·»åŠ RMSå½’ä¸€åŒ–ï¼‰
    for noise_file in tqdm(train_noise_files, desc="ç”Ÿæˆè®­ç»ƒé›†è´Ÿæ ·æœ¬"):
        try:
            audio, sr = sf.read(noise_file)
            
            if sr != sample_rate:
                import librosa
                audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)
            
            if audio.ndim == 2:
                audio = audio.mean(axis=1)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ RMSå½’ä¸€åŒ–ï¼ˆä¸å™ªéŸ³æ± åŠ è½½ä¿æŒä¸€è‡´ï¼‰
            rms = np.sqrt(np.mean(audio**2))
            if rms > 1e-8:
                target_rms = 0.1
                audio = audio * (target_rms / rms)
            
            # å³°å€¼è£å‰ª
            peak = np.max(np.abs(audio))
            if peak > 0.95:
                audio = audio / peak * 0.95
            
            file_id = noise_file.stem
            negative_samples = builder.build_negative_samples(
                audio, file_id, n_negative_per_file_train
            )
            train_negative_samples.extend(negative_samples)
            
        except Exception as e:
            logger.error(f"å¤„ç† {noise_file} æ—¶å‡ºé”™: {e}")
            continue
    
    # ç”ŸæˆéªŒè¯é›†è´Ÿæ ·æœ¬
    for noise_file in tqdm(val_noise_files, desc="ç”ŸæˆéªŒè¯é›†è´Ÿæ ·æœ¬"):
        try:
            audio, sr = sf.read(noise_file)
            
            if sr != sample_rate:
                import librosa
                audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)
            
            if audio.ndim == 2:
                audio = audio.mean(axis=1)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ RMSå½’ä¸€åŒ–
            rms = np.sqrt(np.mean(audio**2))
            if rms > 1e-8:
                target_rms = 0.1
                audio = audio * (target_rms / rms)
            
            # å³°å€¼è£å‰ª
            peak = np.max(np.abs(audio))
            if peak > 0.95:
                audio = audio / peak * 0.95
            
            file_id = noise_file.stem
            negative_samples = builder.build_negative_samples(
                audio, file_id, n_negative_per_file_val
            )
            val_negative_samples.extend(negative_samples)
            
        except Exception as e:
            logger.error(f"å¤„ç† {noise_file} æ—¶å‡ºé”™: {e}")
            continue
    
    logger.info(f"è®­ç»ƒé›†è´Ÿæ ·æœ¬: {len(train_negative_samples)}")
    logger.info(f"éªŒè¯é›†è´Ÿæ ·æœ¬: {len(val_negative_samples)}")
    
    # ========== åˆå¹¶å’Œå¹³è¡¡ ==========
    logger.info(f"\nå¹³è¡¡æ•°æ®é›†...")
    
    # è®­ç»ƒé›†
    all_train_samples = train_samples + train_negative_samples
    balanced_train = builder.balance_dataset(all_train_samples, balance_ratio=balance_ratio)
    
    # éªŒè¯é›†
    all_val_samples = val_positive_samples + val_negative_samples
    balanced_val = builder.balance_dataset(all_val_samples, balance_ratio=balance_ratio)
    
    logger.info(f"å¹³è¡¡åè®­ç»ƒé›†: {len(balanced_train)}")
    logger.info(f"å¹³è¡¡åéªŒè¯é›†: {len(balanced_val)}")
    
    # éªŒè¯æ ·æœ¬å½¢çŠ¶
    unique_lengths = set(len(s['waveform']) for s in balanced_train + balanced_val)
    if len(unique_lengths) > 1:
        logger.error(f"âš ï¸ æ ·æœ¬é•¿åº¦ä¸ä¸€è‡´: {unique_lengths}")
        return
    else:
        logger.info(f"âœ… æ‰€æœ‰æ ·æœ¬é•¿åº¦ç»Ÿä¸€: {list(unique_lengths)[0]} æ ·æœ¬")
    
    # ç»Ÿè®¡
    n_train_pos = sum(1 for s in balanced_train if s['label'] == 1)
    n_train_neg = len(balanced_train) - n_train_pos
    n_val_pos = sum(1 for s in balanced_val if s['label'] == 1)
    n_val_neg = len(balanced_val) - n_val_pos
    
    logger.info(f"\næœ€ç»ˆç»„æˆ:")
    logger.info(f"  è®­ç»ƒé›† - æ­£æ ·æœ¬: {n_train_pos}, è´Ÿæ ·æœ¬: {n_train_neg}")
    logger.info(f"  éªŒè¯é›† - æ­£æ ·æœ¬: {n_val_pos}, è´Ÿæ ·æœ¬: {n_val_neg}")
    
    # ========== ä¿å­˜ ==========
    logger.info(f"\nä¿å­˜åˆ° {output_dir}")
    
    builder.save_dataset(balanced_train, output_dir, split='train')
    builder.save_dataset(balanced_val, output_dir, split='val')
    
    # ========== ä¿å­˜éŸ³é¢‘æ ·æœ¬ç”¨äºéªŒè¯ ==========
    if args.save_wav:
        debug_dir = output_dir / 'debug_wavs'
        debug_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒé›†Click Trainæ ·æœ¬
        if train_samples:
            num_examples = min(10, len(train_samples))
            for i, sample in enumerate(random.sample(train_samples, num_examples)):
                sample_path = debug_dir / f'train_click_train_{i:02d}.wav'
                sf.write(str(sample_path), sample['waveform'], sample_rate)
            logger.info(f"å·²ä¿å­˜ {num_examples} ä¸ªè®­ç»ƒé›†Click Trainæ ·æœ¬")
        
        # ä¿å­˜éªŒè¯é›†Click Trainæ ·æœ¬
        if val_positive_samples:
            num_examples = min(5, len(val_positive_samples))
            for i, sample in enumerate(random.sample(val_positive_samples, num_examples)):
                sample_path = debug_dir / f'val_click_train_{i:02d}.wav'
                sf.write(str(sample_path), sample['waveform'], sample_rate)
            logger.info(f"å·²ä¿å­˜ {num_examples} ä¸ªéªŒè¯é›†Click Trainæ ·æœ¬")
        
        # ä¿å­˜è´Ÿæ ·æœ¬ç¤ºä¾‹
        if train_negative_samples:
            num_examples = min(10, len(train_negative_samples))
            for i, sample in enumerate(random.sample(train_negative_samples, num_examples)):
                sample_path = debug_dir / f'train_noise_{i:02d}.wav'
                sf.write(str(sample_path), sample['waveform'], sample_rate)
            logger.info(f"å·²ä¿å­˜ {num_examples} ä¸ªè®­ç»ƒé›†å™ªå£°æ ·æœ¬")
        
        logger.info(f"è°ƒè¯•éŸ³é¢‘æ ·æœ¬ä¿å­˜åˆ°: {debug_dir}")
    
    # ========== æ€»ç»“ ==========
    logger.info("\n" + "=" * 60)
    logger.info("âœ… æ•°æ®é›†æ„å»ºå®Œæˆ")
    logger.info("=" * 60)
    logger.info(f"æ•°æ®é›†ä¿å­˜åˆ°: {output_dir}")
    logger.info(f"è®­ç»ƒé›†Click Train: {len(train_samples)}")
    logger.info(f"éªŒè¯é›†Click Train: {len(val_positive_samples)}")
    logger.info(f"SNRæ··åˆ: æ‰€æœ‰trainå‡å åŠ æŒç»­èƒŒæ™¯å™ªéŸ³")
    logger.info(f"è®­ç»ƒ/éªŒè¯æœ€ç»ˆæ•°é‡: {len(balanced_train)}/{len(balanced_val)}")
    if args.save_wav:
        logger.info(f"è°ƒè¯•éŸ³é¢‘æ ·æœ¬: {debug_dir}")
    logger.info("=" * 60)

def cmd_train(args):
    """Execute train command (ä¿®æ­£ç‰ˆ - æ·»åŠ æ•°æ®éªŒè¯)."""
    logger = ProjectLogger()
    config = load_config(args.config)
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 60)
    
    # Load dataset
    dataset_dir = Path(args.dataset_dir)
    builder = DatasetBuilder()
    
    logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_dir}")
    
    try:
        train_waveforms, train_labels, train_metadata = builder.load_dataset(dataset_dir / 'train')
        val_waveforms, val_labels, val_metadata = builder.load_dataset(dataset_dir / 'val')
    except FileNotFoundError as e:
        logger.error(f"æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        logger.error(f"è¯·å…ˆè¿è¡Œ: python main.py build-dataset ...")
        return
    
    logger.info(f"è®­ç»ƒé›†æ ·æœ¬: {len(train_waveforms)}")
    logger.info(f"éªŒè¯é›†æ ·æœ¬: {len(val_waveforms)}")
    
    # ğŸ”§ æ–°å¢ï¼šæ•°æ®å½¢çŠ¶éªŒè¯
    logger.info(f"\næ•°æ®éªŒè¯:")
    logger.info(f"  è®­ç»ƒé›†å½¢çŠ¶: {train_waveforms.shape}")
    logger.info(f"  éªŒè¯é›†å½¢çŠ¶: {val_waveforms.shape}")
    
    expected_length = config['model']['input_length']
    
    if train_waveforms.shape[1] != expected_length:
        logger.error(f"âŒ æ•°æ®é›†æ ·æœ¬é•¿åº¦ ({train_waveforms.shape[1]}) ä¸æ¨¡å‹è¾“å…¥é•¿åº¦ ({expected_length}) ä¸åŒ¹é…!")
        logger.error(f"è¯·æ£€æŸ¥:")
        logger.error(f"  1. configs/training.yaml ä¸­ model.input_length æ˜¯å¦ä¸º {train_waveforms.shape[1]}")
        logger.error(f"  2. æ•°æ®é›†æ˜¯å¦æ­£ç¡®æ„å»º")
        return
    
    if val_waveforms.shape[1] != expected_length:
        logger.error(f"âŒ éªŒè¯é›†æ ·æœ¬é•¿åº¦ä¸åŒ¹é…!")
        return
    
    logger.info(f"âœ… æ•°æ®å½¢çŠ¶éªŒè¯é€šè¿‡")
    
    # ğŸ”§ æ–°å¢ï¼šæ ‡ç­¾åˆ†å¸ƒéªŒè¯
    unique_train, counts_train = np.unique(train_labels, return_counts=True)
    unique_val, counts_val = np.unique(val_labels, return_counts=True)
    
    logger.info(f"\næ ‡ç­¾åˆ†å¸ƒ:")
    logger.info(f"  è®­ç»ƒé›† - è´Ÿæ ·æœ¬: {counts_train[0]}, æ­£æ ·æœ¬: {counts_train[1]}")
    logger.info(f"  éªŒè¯é›† - è´Ÿæ ·æœ¬: {counts_val[0]}, æ­£æ ·æœ¬: {counts_val[1]}")
    logger.info(f"  è®­ç»ƒé›†æ­£è´Ÿæ¯”: 1:{counts_train[0]/counts_train[1]:.2f}")
    logger.info(f"  éªŒè¯é›†æ­£è´Ÿæ¯”: 1:{counts_val[0]/counts_val[1]:.2f}")
    
    # ğŸ”§ æ–°å¢ï¼šæ•°æ®èŒƒå›´æ£€æŸ¥ï¼ˆæ£€æµ‹å¼‚å¸¸å€¼ï¼‰
    train_min, train_max = train_waveforms.min(), train_waveforms.max()
    train_mean, train_std = train_waveforms.mean(), train_waveforms.std()
    
    logger.info(f"\næ•°æ®ç»Ÿè®¡:")
    logger.info(f"  è®­ç»ƒé›†èŒƒå›´: [{train_min:.4f}, {train_max:.4f}]")
    logger.info(f"  è®­ç»ƒé›†å‡å€¼: {train_mean:.4f}, æ ‡å‡†å·®: {train_std:.4f}")
    
    if train_max > 10.0 or train_min < -10.0:
        logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸å¤§çš„å¹…åº¦å€¼ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®é—®é¢˜")
    
    # Create data loaders
    logger.info(f"\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    batch_size = config['training']['batch_size']
    
    train_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(
            torch.from_numpy(train_waveforms).float(),
            torch.from_numpy(train_labels).long()
        ),
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # ğŸ”§ CPUè®­ç»ƒæ—¶è®¾ä¸º0
        pin_memory=False
    )
    
    val_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(
            torch.from_numpy(val_waveforms).float(),
            torch.from_numpy(val_labels).long()
        ),
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )
    
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    logger.info(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    logger.info(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # Create model
    logger.info(f"\nåˆ›å»ºæ¨¡å‹...")
    model = create_model(config['model'])
    
    # ğŸ”§ æ‰“å°æ¨¡å‹ä¿¡æ¯
    from models.cnn1d.model import count_parameters
    n_params = count_parameters(model)
    logger.info(f"  æ¨¡å‹ç±»å‹: {'Lightweight' if config['model'].get('use_lightweight', True) else 'Full'}")
    logger.info(f"  æ¨¡å‹å‚æ•°: {n_params:,}")
    logger.info(f"  è¾“å…¥é•¿åº¦: {config['model']['input_length']} æ ·æœ¬")
    
    # Calculate class weights
    class_weights = torch.FloatTensor(len(counts_train) / counts_train)
    logger.info(f"\nç±»åˆ«æƒé‡: {class_weights.tolist()}")
    
    # Initialize trainer
    logger.info(f"\nåˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        device=config['device'],
        learning_rate=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay'],
        class_weights=class_weights
    )
    
    logger.info(f"  è®¾å¤‡: {config['device']}")
    logger.info(f"  å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    logger.info(f"  æƒé‡è¡°å‡: {config['training']['weight_decay']}")
    logger.info(f"  æœ€å¤§è½®æ•°: {config['training']['num_epochs']}")
    logger.info(f"  æ—©åœè€å¿ƒ: {config['training']['early_stopping_patience']}")
    
    # Train
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"\nå¼€å§‹è®­ç»ƒ...")
    logger.info(f"æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {output_dir}")
    logger.info("=" * 60)
    
    try:
        history = trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            num_epochs=config['training']['num_epochs'],
            early_stopping_patience=config['training']['early_stopping_patience'],
            checkpoint_dir=output_dir
        )
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… è®­ç»ƒå®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {min(history['val_loss']):.4f}")
        logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(history['val_acc']):.4f}")
        logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        logger.info("éƒ¨åˆ†æ£€æŸ¥ç‚¹å·²ä¿å­˜")
    except Exception as e:
        logger.error(f"\nâŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


def cmd_eval(args):
    """Execute eval command."""
    logger = ProjectLogger()
    
    logger.info("Evaluating model")
    
    # Load model
    inference = ClickDetectorInference.from_checkpoint(
        args.checkpoint,
        device='cpu',
        batch_size=32
    )
    
    # Load test dataset
    dataset_dir = Path(args.dataset_dir)
    builder = DatasetBuilder()
    test_waveforms, test_labels, _ = builder.load_dataset(dataset_dir)
    
    logger.info(f"Test samples: {len(test_waveforms)}")
    
    # Predict
    y_proba = inference.predict_batch(test_waveforms)
    y_pred = (y_proba >= 0.5).astype(int)
    
    # Generate report
    reporter = EvaluationReporter(Path(args.output_dir))
    
    # Convert to 2D probabilities
    y_proba_2d = np.column_stack([1 - y_proba, y_proba])
    
    generated_files = reporter.generate_report(
        y_true=test_labels,
        y_pred=y_pred,
        y_proba=y_proba_2d,
        metadata={
            'checkpoint': args.checkpoint,
            'dataset': args.dataset_dir
        }
    )
    
    logger.info(f"Evaluation report generated at {args.output_dir}")


def cmd_export(args):
    """Execute export command."""
    logger = ProjectLogger()
    detection_config = load_config('configs/detection.yaml')
    inference_config = load_config(args.config)
    
    logger.info(f"Processing file: {args.input}")
    
    # Load audio
    audio, sr = sf.read(args.input)
    file_id = Path(args.input).stem
    
    # Step 1: Rule-based detection
    params = DetectionParams(
        tkeo_threshold=detection_config['thresholds']['tkeo_z'],
        ste_threshold=detection_config['thresholds']['ste_z'],
        hfc_threshold=detection_config['thresholds']['hfc_z'],
        high_low_ratio_threshold=detection_config['thresholds']['high_low_ratio'],
        envelope_width_min=detection_config['envelope']['width_min_ms'],
        envelope_width_max=detection_config['envelope']['width_max_ms'],
        spectral_centroid_min=detection_config['thresholds']['spectral_centroid_min'],
        refractory_ms=detection_config['refractory_ms']
    )
    
    detector = AdaptiveDetector(sample_rate=sr, params=params)
    candidates = detector.batch_detect(audio)
    
    logger.info(f"Rule-based detection: {len(candidates)} candidates")
    
    # Step 2: Extract 0.2s windows for model inference
    builder = DatasetBuilder(sample_rate=sr)
    windows = []
    
    for candidate in candidates:
        window = builder._extract_centered_window(audio, candidate.peak_idx)
        if window is not None:
            windows.append(window)
        else:
            windows.append(np.zeros(builder.window_samples))
    
    windows = np.array(windows)
    
    # Step 3: Model inference
    inference = ClickDetectorInference.from_checkpoint(
        args.checkpoint,
        device='cpu',
        batch_size=inference_config['batch_size']
    )
    
    model_scores = inference.predict_batch(windows)
    logger.info(f"Model inference completed")
    
    # Step 4: Fusion decision
    fusion_cfg = FusionConfig(
        high_confidence_threshold=inference_config['fusion']['high_confidence_threshold'],
        medium_confidence_threshold=inference_config['fusion']['medium_confidence_threshold'],
        train_consistency_required=inference_config['fusion']['train_consistency_required'],
        min_train_clicks=inference_config['fusion']['min_train_clicks'],
        max_ici_cv=inference_config['fusion']['max_ici_cv'],
        doublet_min_ici_ms=inference_config['doublet']['min_ici_ms'],
        doublet_max_ici_ms=inference_config['doublet']['max_ici_ms'],
        doublet_min_confidence=inference_config['doublet']['min_confidence']
    )
    
    decider = FusionDecider(config=fusion_cfg)
    accepted_indices, decision_info = decider.apply_fusion(candidates, model_scores)
    
    accepted_candidates = [candidates[i] for i in accepted_indices]
    
    logger.info(f"Fusion decision: {len(accepted_candidates)} accepted")
    logger.info(decider.get_statistics(decision_info))
    
    # Step 5: Build trains
    train_builder = TrainBuilder(
        min_ici_ms=detection_config['train']['min_ici_ms'],
        max_ici_ms=detection_config['train']['max_ici_ms'],
        min_train_clicks=detection_config['train']['min_train_clicks']
    )
    
    trains = train_builder.build_trains(accepted_candidates)
    logger.info(f"Built {len(trains)} click trains")
    
    # Step 6: Export results
    output_dir = Path(args.output_dir)
    exporter = ExportWriter(output_dir, sample_rate=sr)
    
    if inference_config['export']['export_events']:
        event_files = exporter.export_events(
            accepted_candidates,
            audio,
            file_id,
            export_audio=inference_config['export']['export_audio']
        )
        logger.info(f"Events exported to {event_files['csv']}")
    
    if inference_config['export']['export_trains']:
        train_files = exporter.export_trains(
            trains,
            accepted_candidates,
            audio,
            file_id,
            export_audio=inference_config['export']['export_audio']
        )
        logger.info(f"Trains exported to {train_files['csv']}")
    
    if inference_config['export']['create_summary']:
        summary_stats = {
            'total_candidates': len(candidates),
            'accepted_clicks': len(accepted_candidates),
            'rejection_rate': 1 - len(accepted_candidates) / len(candidates) if candidates else 0,
            'num_trains': len(trains),
            **decision_info
        }
        
        report_path = exporter.create_summary_report(file_id, summary_stats)
        logger.info(f"Summary report: {report_path}")
    
    logger.info("Export completed successfully")

def cmd_eval_wav(args):
    """
    æ‰§è¡Œeval-wavå‘½ä»¤ - è¯„ä¼°500mséŸ³é¢‘ç‰‡æ®µ
    
    å·¥ä½œæµç¨‹:
    1. åŠ è½½æ¨¡å‹
    2. æ‰«ææ­£/è´Ÿæ ·æœ¬ç›®å½•
    3. æ‰¹é‡æ¨ç†
    4. è®¡ç®—æŒ‡æ ‡å¹¶ç”ŸæˆæŠ¥å‘Š
    """
    logger = ProjectLogger()
    config = load_config(args.config)
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹WAVæ–‡ä»¶è¯„ä¼°(500msç‰‡æ®µæ¨¡å¼)")
    logger.info("=" * 60)
    
    # ========== 1. åŠ è½½æ¨¡å‹ ==========
    logger.info(f"\nåŠ è½½æ¨¡å‹: {args.checkpoint}")
    try:
        inference = ClickDetectorInference.from_checkpoint(
            args.checkpoint,
            device=config['inference']['device'],
            batch_size=config['inference']['batch_size']
        )
        logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # ========== 2. æ‰«ææµ‹è¯•é›†æ–‡ä»¶ ==========
    positive_dir = Path(args.positive_dir)
    negative_dir = Path(args.negative_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"\næ‰«ææµ‹è¯•é›†æ–‡ä»¶...")
    logger.info(f"  æ­£æ ·æœ¬ç›®å½•: {positive_dir}")
    logger.info(f"  è´Ÿæ ·æœ¬ç›®å½•: {negative_dir}")
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰wavæ–‡ä»¶
    positive_files = list(positive_dir.rglob('*.wav'))
    negative_files = list(negative_dir.rglob('*.wav'))
    
    logger.info(f"  æ‰¾åˆ°æ­£æ ·æœ¬: {len(positive_files)} ä¸ª")
    logger.info(f"  æ‰¾åˆ°è´Ÿæ ·æœ¬: {len(negative_files)} ä¸ª")
    
    if len(positive_files) == 0 or len(negative_files) == 0:
        logger.error("âŒ æµ‹è¯•é›†æ–‡ä»¶æ•°é‡ä¸è¶³!")
        return
    
    # ========== 3. åŠ è½½å¹¶é¢„å¤„ç†éŸ³é¢‘ ==========
    logger.info(f"\nåŠ è½½éŸ³é¢‘æ–‡ä»¶...")
    
    def load_audio_files(file_list, label, sample_rate=44100):
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶ç»Ÿä¸€é•¿åº¦"""
        waveforms = []
        labels = []
        file_paths = []
        
        for wav_file in tqdm(file_list, desc=f"åŠ è½½{label}æ ·æœ¬"):
            try:
                audio, sr = sf.read(wav_file)
                
                # é‡é‡‡æ ·
                if sr != sample_rate:
                    import librosa
                    audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)
                
                # è½¬å•å£°é“
                if audio.ndim == 2:
                    audio = audio.mean(axis=1)
                
                # ç»Ÿä¸€é•¿åº¦åˆ°500ms (22050æ ·æœ¬)
                target_length = int(0.5 * sample_rate)
                if len(audio) > target_length:
                    audio = audio[:target_length]
                elif len(audio) < target_length:
                    # ä¸­å¿ƒpadding
                    pad_total = target_length - len(audio)
                    pad_left = pad_total // 2
                    pad_right = pad_total - pad_left
                    audio = np.pad(audio, (pad_left, pad_right), mode='constant')
                
                waveforms.append(audio)
                labels.append(label)
                file_paths.append(str(wav_file))
                
            except Exception as e:
                logger.error(f"åŠ è½½å¤±è´¥ {wav_file}: {e}")
                continue
        
        return np.array(waveforms), np.array(labels), file_paths
    
    # åŠ è½½æ­£æ ·æœ¬(label=1)
    pos_waveforms, pos_labels, pos_paths = load_audio_files(
        positive_files, label=1
    )
    
    # åŠ è½½è´Ÿæ ·æœ¬(label=0)
    neg_waveforms, neg_labels, neg_paths = load_audio_files(
        negative_files, label=0
    )
    
    # åˆå¹¶
    all_waveforms = np.vstack([pos_waveforms, neg_waveforms])
    all_labels = np.concatenate([pos_labels, neg_labels])
    all_paths = pos_paths + neg_paths
    
    logger.info(f"\næ•°æ®åŠ è½½å®Œæˆ:")
    logger.info(f"  æ€»æ ·æœ¬æ•°: {len(all_waveforms)}")
    logger.info(f"  æ­£æ ·æœ¬: {np.sum(all_labels == 1)}")
    logger.info(f"  è´Ÿæ ·æœ¬: {np.sum(all_labels == 0)}")
    logger.info(f"  æ ·æœ¬å½¢çŠ¶: {all_waveforms.shape}")
    
    # ========== 4. æ¨¡å‹æ¨ç† ==========
    logger.info(f"\nå¼€å§‹æ¨¡å‹æ¨ç†...")
    
    try:
        y_proba = inference.predict_batch(all_waveforms)
        logger.info(f"âœ… æ¨ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # è·å–ç½®ä¿¡åº¦é˜ˆå€¼
    threshold = config['thresholds']['confidence_threshold']
    y_pred = (y_proba >= threshold).astype(int)
    
    logger.info(f"  ä½¿ç”¨é˜ˆå€¼: {threshold}")
    logger.info(f"  é¢„æµ‹ä¸ºæ­£æ ·æœ¬: {np.sum(y_pred == 1)}")
    logger.info(f"  é¢„æµ‹ä¸ºè´Ÿæ ·æœ¬: {np.sum(y_pred == 0)}")
    
    # ========== 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ==========
    logger.info(f"\nè®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # åŸºç¡€æŒ‡æ ‡
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(all_labels, y_proba)
    roc_auc = auc(fpr, tpr)
    
    # PRæ›²çº¿
    precision_curve, recall_curve, _ = precision_recall_curve(all_labels, y_proba)
    pr_auc = auc(recall_curve, precision_curve)
    
    # æ‰“å°ç»“æœ
    logger.info(f"\n" + "=" * 60)
    logger.info("è¯„ä¼°ç»“æœ")
    logger.info("=" * 60)
    logger.info(f"å‡†ç¡®ç‡ (Accuracy):  {accuracy:.4f}")
    logger.info(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    logger.info(f"å¬å›ç‡ (Recall):    {recall:.4f}")
    logger.info(f"F1åˆ†æ•° (F1 Score):  {f1:.4f}")
    logger.info(f"ROC AUC:            {roc_auc:.4f}")
    logger.info(f"PR AUC:             {pr_auc:.4f}")
    logger.info(f"\næ··æ·†çŸ©é˜µ:")
    logger.info(f"  TN: {tn:<6} FP: {fp}")
    logger.info(f"  FN: {fn:<6} TP: {tp}")
    logger.info("=" * 60)
    
    # ========== 6. ä¿å­˜ç»“æœ ==========
    logger.info(f"\nä¿å­˜è¯„ä¼°ç»“æœåˆ°: {output_dir}")
    
    # 6.1 ä¿å­˜é¢„æµ‹ç»“æœCSV
    if config['output']['save_predictions']:
        results_df = pd.DataFrame({
            'file_path': all_paths,
            'true_label': all_labels,
            'predicted_label': y_pred,
            'confidence': y_proba,
            'correct': (all_labels == y_pred)
        })
        results_csv = output_dir / 'predictions.csv'
        results_df.to_csv(results_csv, index=False)
        logger.info(f"  âœ… é¢„æµ‹ç»“æœ: {results_csv}")
    
    # 6.2 ä¿å­˜è¯¯åˆ†ç±»æ–‡ä»¶
    if config['output']['save_misclassified_files']:
        misclassified = results_df[~results_df['correct']]
        if len(misclassified) > 0:
            misc_csv = output_dir / 'misclassified.csv'
            misclassified.to_csv(misc_csv, index=False)
            logger.info(f"  âœ… è¯¯åˆ†ç±»æ–‡ä»¶: {misc_csv} ({len(misclassified)}ä¸ª)")
    
    # 6.3 ä¿å­˜æ··æ·†çŸ©é˜µå›¾
    if config['output']['save_confusion_matrix']:
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Negative', 'Positive'],
                   yticklabels=['Negative', 'Positive'])
        plt.title(f'Confusion Matrix (Acc: {accuracy:.3f})')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        cm_path = output_dir / 'confusion_matrix.png'
        plt.savefig(cm_path, dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"  âœ… æ··æ·†çŸ©é˜µå›¾: {cm_path}")
    
    # 6.4 ä¿å­˜ROCæ›²çº¿
    if config['output']['save_roc_curve']:
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.grid(alpha=0.3)
        roc_path = output_dir / 'roc_curve.png'
        plt.savefig(roc_path, dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"  âœ… ROCæ›²çº¿: {roc_path}")
    
    # 6.5 ä¿å­˜PRæ›²çº¿
    if config['output']['save_pr_curve']:
        plt.figure(figsize=(8, 6))
        plt.plot(recall_curve, precision_curve, linewidth=2,
                label=f'PR (AUC = {pr_auc:.3f})')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.legend()
        plt.grid(alpha=0.3)
        pr_path = output_dir / 'pr_curve.png'
        plt.savefig(pr_path, dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"  âœ… PRæ›²çº¿: {pr_path}")
    
    # 6.6 ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    if config['output']['generate_detailed_report']:
        report_path = output_dir / 'evaluation_report.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("æ¨¡å‹è¯„ä¼°æŠ¥å‘Š - 500msç‰‡æ®µæ¨¡å¼\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"æ¨¡å‹: {args.checkpoint}\n")
            f.write(f"æ­£æ ·æœ¬ç›®å½•: {positive_dir}\n")
            f.write(f"è´Ÿæ ·æœ¬ç›®å½•: {negative_dir}\n")
            f.write(f"ç½®ä¿¡åº¦é˜ˆå€¼: {threshold}\n\n")
            
            f.write("æ•°æ®é›†ç»Ÿè®¡:\n")
            f.write(f"  æ€»æ ·æœ¬æ•°: {len(all_waveforms)}\n")
            f.write(f"  æ­£æ ·æœ¬: {np.sum(all_labels == 1)}\n")
            f.write(f"  è´Ÿæ ·æœ¬: {np.sum(all_labels == 0)}\n\n")
            
            f.write("è¯„ä¼°æŒ‡æ ‡:\n")
            f.write(f"  å‡†ç¡®ç‡:  {accuracy:.4f}\n")
            f.write(f"  ç²¾ç¡®ç‡:  {precision:.4f}\n")
            f.write(f"  å¬å›ç‡:  {recall:.4f}\n")
            f.write(f"  F1åˆ†æ•°:  {f1:.4f}\n")
            f.write(f"  ROC AUC: {roc_auc:.4f}\n")
            f.write(f"  PR AUC:  {pr_auc:.4f}\n\n")
            
            f.write("æ··æ·†çŸ©é˜µ:\n")
            f.write(f"  çœŸè´Ÿä¾‹(TN): {tn}\n")
            f.write(f"  å‡æ­£ä¾‹(FP): {fp}\n")
            f.write(f"  å‡è´Ÿä¾‹(FN): {fn}\n")
            f.write(f"  çœŸæ­£ä¾‹(TP): {tp}\n\n")
            
            f.write("sklearnåˆ†ç±»æŠ¥å‘Š:\n")
            f.write(classification_report(
                all_labels, y_pred,
                target_names=['Negative', 'Positive']
            ))
        
        logger.info(f"  âœ… è¯¦ç»†æŠ¥å‘Š: {report_path}")
    
    logger.info(f"\n" + "=" * 60)
    logger.info("âœ… è¯„ä¼°å®Œæˆ!")
    logger.info("=" * 60)


def main():
    """Main entry point."""
    parser = setup_argparse()
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        sys.exit(1)
    
    # Route command
    commands = {
        'scan': cmd_scan,
        'detect': cmd_detect,
        'batch-detect': cmd_batch_detect,
        'collect-clicks': cmd_collect_clicks,  # Added
        'trains': cmd_trains,
        'build-dataset': cmd_build_dataset,    # Fixed
        'train': cmd_train,
        'eval': cmd_eval,
        'eval-wav': cmd_eval_wav,
        'export': cmd_export
    }
    
    try:
        commands[args.command](args)
    except Exception as e:
        logger = ProjectLogger()
        logger.error(f"Command failed: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()